{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in raw temperature data (df1) and grid (df2) from steps 1 and 2\n",
    "\n",
    "- Need to run run.py to populate results directory with step[1,2]_output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1_path = \"results/step1_output.csv\"\n",
    "step2_path = \"results/step2_output.csv\"\n",
    "\n",
    "df1 = pd.read_csv(step1_path, index_col='Station_ID')\n",
    "df2 = pd.read_csv(step2_path, index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridding V1: Evaluate if series_next has 20 points of overlap with series_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridding_v1_pandas(df1, df2):\n",
    "\n",
    "    missing_df_month = []\n",
    "    missing_df_month_list = []\n",
    "\n",
    "    # Loop through each point in 2x2 latxlon grid\n",
    "    point_series_list = []\n",
    "    for point in tqdm(range(len(df2))):\n",
    "\n",
    "        # Create dataframe for all stations within range of given point\n",
    "        nearby_station_str = str(df2.iloc[point]['Nearby_Stations'])\n",
    "        nearby_station_dict = json.loads(nearby_station_str.replace(\"'\", '\"'))\n",
    "        df_point = df1.loc[df1.index.isin(nearby_station_dict.keys())]\n",
    "\n",
    "        # Loop through all months\n",
    "        df_month_list = []\n",
    "        for month in range(1, 13):\n",
    "    \n",
    "            # Filter months\n",
    "            df_month = df_point.filter(regex=f'^{month}_')\n",
    "\n",
    "            # Save original column names for later use\n",
    "            original_columns = df_month.columns.tolist()\n",
    "\n",
    "            # Add weights column\n",
    "            df_month['Weight'] = df_month.index.map(lambda station_id: nearby_station_dict.get(station_id))\n",
    "\n",
    "            # Sort stations by number of points, drop rows with less than 20 data points\n",
    "            df_month['Valid Data'] = df_month.filter(regex='_').count(axis=1)\n",
    "            df_month = df_month[df_month['Valid Data'] >= 20]\n",
    "            df_month = df_month.sort_values(by='Valid Data', ascending=False)\n",
    "\n",
    "            # Calculate average per station if not already provided\n",
    "            station_averages = df_month.filter(regex='_').mean(axis=1)\n",
    "\n",
    "            # Only get monthly series if there are values\n",
    "            if len(df_month) > 0:\n",
    "\n",
    "                df_month_temp = df_month.filter(regex='_')\n",
    "                weights = df_month['Weight']\n",
    "\n",
    "                # Initialize combined series and weights\n",
    "                series_combined = df_month_temp.iloc[0]\n",
    "                weight_combined = weights.iloc[0]\n",
    "\n",
    "                #Initialize mean, avoid recalculating in each loop\n",
    "                series_combined_mean = series_combined.mean() if series_combined.count() > 0 else float('nan')\n",
    "    \n",
    "                for i in range(1, len(df_month_temp)):\n",
    "                    series_next = df_month_temp.iloc[i]\n",
    "                    weight_next = weights[i]\n",
    "                    station_next_avg = station_averages.iloc[i]\n",
    "\n",
    "                    ################################################################################\n",
    "                    ### Only use series_next if it has 20 points of overlap with series_combined ###\n",
    "                    ################################################################################\n",
    "                    valid_combined = ~series_combined.isnull()\n",
    "                    valid_next = ~series_next.isnull()\n",
    "                    valid_indices = valid_combined & valid_next\n",
    "                    overlapping_count = valid_indices.sum()\n",
    "\n",
    "                    # Only evaluate if overlapping count is over threshold (should be 20)\n",
    "                    if overlapping_count > 20:\n",
    "\n",
    "                        # Calculate the new mean for combined series before updating\n",
    "                        new_combined_mean = (series_combined_mean * weight_combined + station_next_avg * weight_next) / (weight_combined + weight_next)\n",
    "\n",
    "                        # Adjust series_next using averages\n",
    "                        series_next_adjusted = series_next + station_next_avg - series_combined_mean\n",
    "\n",
    "                        # Update series_combined\n",
    "                        series_combined[~series_next_adjusted.isnull()] += series_next_adjusted[~series_next_adjusted.isnull()] * weight_next\n",
    "\n",
    "                        # Update combined weight\n",
    "                        weight_combined += weight_next\n",
    "\n",
    "                        # Update the combined mean\n",
    "                        series_combined_mean = new_combined_mean\n",
    "\n",
    "                # Add to list of monthly dataframes\n",
    "                df_month_list.append(series_combined)\n",
    "\n",
    "            else:\n",
    "                point_month = str(point) + '_' + str(month)\n",
    "                missing_df_month.append(point_month)\n",
    "                \n",
    "\n",
    "        if len(df_month_list) > 0:\n",
    "            # Combine into series for all months, change index to point number\n",
    "            complete_point_series = pd.concat(df_month_list, axis=0)\n",
    "            complete_point_series.name = point\n",
    "\n",
    "        else:\n",
    "            missing_df_month_list.append(point)\n",
    "\n",
    "        # Add complete point series to list\n",
    "        point_series_list.append(complete_point_series)\n",
    "\n",
    "\n",
    "    # Create dataframe of gridded time series for each point\n",
    "    df_gridded = pd.concat(point_series_list, axis=1).T\n",
    "\n",
    "    # Reorder columns to be in time-order\n",
    "    ordered_columns = []\n",
    "    for year in range(1880, 2024):\n",
    "        for month in range(1, 13):\n",
    "            ordered_columns.append(f'{month}_{year}')\n",
    "    df_gridded = df_gridded[ordered_columns]\n",
    "\n",
    "\n",
    "    # Add latitude / longitude coordinates\n",
    "    df_gridded[['Latitude', 'Longitude']] = df2[['Latitude', 'Longitude']]\n",
    "\n",
    "    # Clean out rows that should be all NaN\n",
    "    df_clean = df_gridded.copy()\n",
    "    anom_columns = df_clean.columns[:-2]\n",
    "    for point in missing_df_month_list:\n",
    "        df_clean.iloc[point][anom_columns] = np.nan\n",
    "    for point_month in missing_df_month:\n",
    "        point = int(point_month.split('_')[0])\n",
    "        month = point_month.split('_')[1]\n",
    "        month_cols = []\n",
    "        for year in range(1880, 2024):\n",
    "            month_cols.append((str(month) + '_' + str(year)))\n",
    "        df_clean.iloc[point][month_cols] = np.nan\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–‰         | 1442/16022 [08:04<8:55:12,  2.20s/it] "
     ]
    }
   ],
   "source": [
    "df_gridded = gridding_v1_pandas(df1, df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Calculation: anomaly = raw - baseline_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_anomalies(df_gridded):\n",
    "\n",
    "    # Create dataframe for baseline columns\n",
    "    df_baseline = df_gridded.copy()\n",
    "    baseline_columns = []\n",
    "    for year in range(1951, 1981):\n",
    "        for month in range(1, 13):\n",
    "            baseline_columns.append(str(month) + '_' + str(year))\n",
    "    df_baseline = df_baseline[baseline_columns]\n",
    "\n",
    "    # Calculate anomalies for each month\n",
    "    df_monthly_anom_list = []\n",
    "    for month in range(1, 13):\n",
    "\n",
    "        # Get monthly dataframes and baseline averages\n",
    "        monthly_columns = [col for col in df_gridded.columns if col.startswith(f'{month}_')]\n",
    "        df_monthly_anom = df_gridded[monthly_columns]\n",
    "        df_baseline[f'{month}_Average'] = df_baseline.mean(axis=1)\n",
    "        df_monthly_anom[f'{month}_Average'] = df_baseline.mean(axis=1)\n",
    "\n",
    "        # Calculate anomalies for each column\n",
    "        for col in df_monthly_anom.columns:\n",
    "            df_monthly_anom[col] = df_monthly_anom[col] - df_monthly_anom[f'{month}_Average']\n",
    "\n",
    "        # Drop baseline average column, add to list\n",
    "        df_monthly_anom = df_monthly_anom.drop(columns=[f'{month}_Average'])\n",
    "        df_monthly_anom_list.append(df_monthly_anom)\n",
    "\n",
    "    # Create dataframe and sort columns\n",
    "    df_anom = pd.concat(df_monthly_anom_list, axis=1)\n",
    "    temp_columns = []\n",
    "    for year in range(1880, 2024):\n",
    "        for month in range(1, 13):\n",
    "            temp_columns.append(str(month) + '_' + str(year))\n",
    "    df_anom = df_anom[temp_columns]\n",
    "\n",
    "    # Add Latitude / Longitude coordinates\n",
    "    lat_lon_columns = df_gridded[['Latitude', 'Longitude']].copy()\n",
    "    df_anom = pd.concat([df_anom, lat_lon_columns], axis=1)\n",
    "\n",
    "    return df_anom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert dataframes to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_dataset(grid_anomaly: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Convert a DataFrame with temperature data into an xarray Dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - grid_anomaly (DataFrame): Input DataFrame with temperature data, containing columns 'Lat', 'Lon', and columns representing time steps.\n",
    "\n",
    "    Returns:\n",
    "    - Dataset: xarray Dataset with temperature data, indexed by latitude, longitude, and time.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create copy of input dataframe, rename columns\n",
    "    df = grid_anomaly.copy()\n",
    "    df = df.rename(columns={\"Latitude\": \"lat\", \"Longitude\": \"lon\"})\n",
    "\n",
    "    # Reshape dataframe into long format\n",
    "    df = df.melt(id_vars=[\"lat\", \"lon\"], var_name=\"date\", value_name=\"temp\")\n",
    "\n",
    "    # Get months and years, drop duplicate rows\n",
    "    df[[\"month\", \"year\"]] = df[\"date\"].str.split(\"_\", expand=True).astype(int)\n",
    "    df = df.drop_duplicates(subset=[\"lat\", \"lon\", \"month\", \"year\"])\n",
    "\n",
    "    # Create date column formatted as year-month-01\n",
    "    dates = df[\"year\"].astype(str) + \"-\" + df[\"month\"].astype(str) + \"-01\"\n",
    "\n",
    "    # Convert dates to datetime objects\n",
    "    datetimes = pd.to_datetime(dates)\n",
    "\n",
    "    # Remove unnecessary columns\n",
    "    df = df.drop(columns=[\"date\", \"month\", \"year\"])\n",
    "\n",
    "    # Create new time column using datetime objects\n",
    "    df[\"time\"] = pd.to_datetime(datetimes)\n",
    "\n",
    "    # Set multi-index\n",
    "    df = df.set_index([\"lat\", \"lon\", \"time\"])\n",
    "\n",
    "    # Convert pandas dataframe to xarray dataset\n",
    "    ds = df.to_xarray()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equal Area Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_weighted_timeseries(ds):\n",
    "    weights = np.cos(np.deg2rad(ds['lat']))\n",
    "    weights.name = 'weights'\n",
    "    ds_weighted = ds.temp.weighted(weights)\n",
    "    weighted_mean = ds_weighted.mean(('lon', 'lat'))\n",
    "    yearly_weighted_timeseries = weighted_mean.resample(time='Y').mean(dim='time')\n",
    "    return yearly_weighted_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timeseries_dataset(df_gridded):\n",
    "    df_anomaly = calculate_anomalies(df_gridded)\n",
    "    ds = dataframe_to_dataset(df_anomaly)\n",
    "    timeseries = area_weighted_timeseries(ds)\n",
    "    return timeseries, ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get baseline results using gridding version 0 function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_gridded.copy()\n",
    "df_anomaly = calculate_anomalies(df)\n",
    "timeseries, ds = get_timeseries_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting: Comparison of V4 & V5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create v4 anomaly dataframe (steps 0-3)\n",
    "df_v4 = pd.read_csv('df_v4.csv', index_col='Unnamed: 0')\n",
    "\n",
    "# Create global average timeseries for v4\n",
    "df_annual = df_v4.reindex(sorted(df_v4.columns), axis=1)\n",
    "df_v4_timeseries = df_annual.mean(axis=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_v4_vs_v5(timeseries, title, color):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.patch.set_facecolor(color)\n",
    "\n",
    "    plt.plot(timeseries, label='v5')\n",
    "    plt.plot(df_v4_timeseries, label='v4')\n",
    "    plt.axvline(x=70)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Temperature Anomaly (Celsius)')\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_v4_vs_v5(timeseries, 'Gridding V0 Cleaned', 'yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_plot(ds, index):\n",
    "    most_recent_time = ds.time[index]\n",
    "    temp_values = ds.temp.sel(time=most_recent_time)\n",
    "    lon, lat = np.meshgrid(ds.lon, ds.lat)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.contourf(lon, lat, temp_values, cmap='coolwarm')\n",
    "    plt.colorbar(label='Temperature (Â°C)')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.title(f'Most Recent Temperature Values ({most_recent_time})')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_plot(ds, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
